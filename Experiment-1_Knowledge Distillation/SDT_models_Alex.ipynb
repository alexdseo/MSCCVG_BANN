{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LG\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as tfk\n",
    "from tensorflow.keras.initializers import RandomNormal, TruncatedNormal\n",
    "from tensorflow.keras.layers import (Input, Dense, Activation, Layer, Lambda,\n",
    "                                     Concatenate)\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainableVar(Layer):\n",
    "    '''Creates variable that's trainable with keras model. Needs to be attached\n",
    "    to some node that conditions optimizer op.'''\n",
    "    def __init__(self, name, shape, **kwargs):\n",
    "        super(TrainableVar, self).__init__()\n",
    "        self.kernel = self.add_variable(name=name, shape=shape, **kwargs)\n",
    "\n",
    "    def call(self, input):\n",
    "        return self.kernel\n",
    "\n",
    "class LeftBranch(Layer):\n",
    "    def call(self, input):\n",
    "        return input[0] * (1 - input[1])\n",
    "\n",
    "class RightBranch(Layer):\n",
    "    def call(self, input):\n",
    "        return input[0] * input[1]\n",
    "\n",
    "class Scale(Layer):\n",
    "    def call(self, input):\n",
    "        return input[0] * input[1]\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, id, depth, pathprob, tree):\n",
    "        self.id = id\n",
    "        self.depth = depth\n",
    "        self.pathprob = pathprob\n",
    "        self.isLeaf = self.depth == tree.max_depth\n",
    "        self.leftChild = None\n",
    "        self.rightChild = None\n",
    "\n",
    "        if self.isLeaf:\n",
    "            self.phi = TrainableVar(\n",
    "                name='phi_'+self.id, shape=(1, tree.n_classes),\n",
    "                dtype='float32', initializer=TruncatedNormal())(pathprob)\n",
    "        else:\n",
    "            self.dense = Dense(\n",
    "                units=1, name='dense_'+self.id, dtype='float32',\n",
    "                kernel_initializer=RandomNormal(),\n",
    "                bias_initializer=TruncatedNormal())(tree.input_layer)\n",
    "\n",
    "    def build(self, tree):\n",
    "        '''Defines the output probability of the node and builds child nodes.'''\n",
    "        self.prob = self.forward(tree)\n",
    "        if not self.isLeaf:\n",
    "            leftprob = LeftBranch()([self.pathprob, self.prob])\n",
    "            rightprob = RightBranch()([self.pathprob, self.prob])\n",
    "            self.leftChild = Node(id=self.id+'0', depth=self.depth+1,\n",
    "                                  pathprob=leftprob, tree=tree)\n",
    "            self.rightChild = Node(id=self.id+'1', depth=self.depth+1,\n",
    "                                   pathprob=rightprob, tree=tree)\n",
    "\n",
    "    def forward(self, tree):\n",
    "        '''Defines the output probability.'''\n",
    "        if not self.isLeaf:\n",
    "            self.dense_scaled = Scale()([tree.inv_temp, self.dense])\n",
    "            return Activation('sigmoid', name='prob_' + self.id)(\n",
    "                self.dense_scaled)\n",
    "        else:\n",
    "            return Activation('softmax', name='pdist_' + self.id)(self.phi)\n",
    "\n",
    "    def get_penalty(self, tree):\n",
    "        '''From paper: \"... we can maintain an exponentially decaying running\n",
    "        average of the actual probabilities with a time window that is\n",
    "        exponentially proportional to the depth of the node.\"\n",
    "        So here we track EMAs of batches of P^i and p_i and calculate:\n",
    "            alpha = sum(ema(P^i) * ema(p_i)) / sum(ema(P^i))\n",
    "            penalty = -0.5 * (log(alpha) + log(1-alpha))\n",
    "        '''\n",
    "        # Keep track of running average of probabilities (batch-wise)\n",
    "        # with exponential growth of time window w.r.t. the  depth of the node\n",
    "        self.ema = tf.train.ExponentialMovingAverage(\n",
    "            decay=0.9999, num_updates=tree.ema_win_size*2**self.depth)\n",
    "        self.ema_apply_op = self.ema.apply([self.pathprob, self.prob])\n",
    "        self.ema_P = self.ema.average(self.pathprob)\n",
    "        self.ema_p = self.ema.average(self.prob)\n",
    "        # Calculate alpha by summing probs and pathprobs over batch\n",
    "        self.alpha = (tf.reduce_sum(self.ema_P * self.ema_p) + tree.eps) / (\n",
    "            tf.reduce_sum(self.ema_P) + tree.eps)\n",
    "        # Calculate penalty for this node using running average of alpha\n",
    "        self.penalty = (- 0.5 * tf.log(self.alpha + tree.eps)\n",
    "                        - 0.5 * tf.log(1. - self.alpha + tree.eps))\n",
    "        # Replace possible NaN values with zeros\n",
    "        self.penalty = tf.where(\n",
    "            tf.is_nan(self.penalty), tf.zeros_like(self.penalty), self.penalty)\n",
    "        return self.penalty\n",
    "\n",
    "    def get_loss(self, y, tree):\n",
    "        if self.isLeaf:\n",
    "            # Cross-entropies (batch) of soft labels with output of this leaf\n",
    "            leaf_ce = - tf.reduce_sum(y * tf.log(tree.eps + self.prob), axis=1)\n",
    "            # Mean of cross-entropies weighted by path probability of this leaf\n",
    "            self.leaf_loss = tf.reduce_mean(self.pathprob * leaf_ce)\n",
    "            # Return leaf contribution to the loss\n",
    "            return self.leaf_loss\n",
    "        else:\n",
    "            # Return decayed penalty term of this (inner) node\n",
    "            return tree.penalty_strength * self.get_penalty(tree) * (\n",
    "                tree.penalty_decay**self.depth) # decay\n",
    "\n",
    "\n",
    "class Constant(Layer):\n",
    "    def __init__(self, value=1, **kwargs):\n",
    "        self.value = value\n",
    "        super(Constant, self).__init__(**kwargs)\n",
    "    def call(self, input):\n",
    "        return tfk.constant(self.value, shape=(1,), dtype='float32')\n",
    "\n",
    "\n",
    "class OutputLayer(Layer):\n",
    "    def call(self, input):\n",
    "        opinions, weights = input\n",
    "        opinions = Concatenate(axis=0)(opinions) # shape=(n_bigots,n_classes)\n",
    "        weights = Concatenate(axis=1)(weights) # shape=(batch_size,n_bigots)\n",
    "        elems = tfk.argmax(weights, axis=1) # shape=(batch_size,)\n",
    "        def from_keras_tensor(opinions, elems=None):\n",
    "            return tfk.map_fn(lambda x: opinions[x], elems, dtype=tf.float32)\n",
    "        outputs = Lambda(\n",
    "            from_keras_tensor, arguments={'elems': elems})(opinions)\n",
    "        return outputs # shape=(batch_size,n_classes)\n",
    "\n",
    "\n",
    "class SoftBinaryDecisionTree(object):\n",
    "    def __init__(self, max_depth, n_features, n_classes,\n",
    "                 penalty_strength=10.0, penalty_decay=0.5, inv_temp=0.01,\n",
    "                 ema_win_size=100, learning_rate=3e-4, metrics=['acc']):\n",
    "        '''Initialize model instance by saving parameter values\n",
    "        as model properties and creating others as placeholders.\n",
    "        '''\n",
    "\n",
    "        # save hyperparameters\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.penalty_strength = penalty_strength\n",
    "        self.penalty_decay = penalty_decay\n",
    "        self.inv_temp = inv_temp\n",
    "        self.ema_win_size = ema_win_size\n",
    "\n",
    "        self.nodes = list()\n",
    "        self.bigot_opinions = list()\n",
    "        self.bigot_weights = list()\n",
    "        self.ema_apply_ops = list()\n",
    "\n",
    "        self.loss = 0.0\n",
    "        self.loss_leaves = 0.0\n",
    "        self.loss_penalty = 0.0\n",
    "\n",
    "        self.optimizer = Adam(lr=learning_rate)\n",
    "        self.metrics = metrics\n",
    "\n",
    "        self.eps = tfk.constant(1e-8, shape=(1,), dtype='float32')\n",
    "        self.initialized = False\n",
    "\n",
    "    def build_model(self):\n",
    "        self.input_layer = Input(shape=(self.n_features,), dtype='float32')\n",
    "\n",
    "        if self.inv_temp:\n",
    "            self.inv_temp = Constant(value=self.inv_temp)(self.input_layer)\n",
    "        else:\n",
    "            self.inv_temp = TrainableVar(\n",
    "                name='beta', shape=(1,), dtype='float32',\n",
    "                initializer=RandomNormal())(self.input_layer)\n",
    "\n",
    "        self.root = Node(\n",
    "            id='0', depth=0, pathprob=Constant()(self.input_layer), tree=self)\n",
    "        self.nodes.append(self.root)\n",
    "\n",
    "        for node in self.nodes:\n",
    "            node.build(tree=self)\n",
    "            if node.isLeaf:\n",
    "                self.bigot_opinions.append(node.prob)\n",
    "                self.bigot_weights.append(node.pathprob)\n",
    "            else:\n",
    "                self.nodes.append(node.leftChild)\n",
    "                self.nodes.append(node.rightChild)\n",
    "\n",
    "        def tree_loss(y_true, y_pred):\n",
    "            for node in self.nodes:\n",
    "                if node.isLeaf:\n",
    "                    self.loss_leaves += node.get_loss(y=y_true, tree=self)\n",
    "                else:\n",
    "                    self.loss_penalty += node.get_loss(y=None, tree=self)\n",
    "                    self.ema_apply_ops.append(node.ema_apply_op)\n",
    "\n",
    "            with tf.control_dependencies(self.ema_apply_ops):\n",
    "                self.loss = tf.log(\n",
    "                    self.eps + self.loss_leaves) + self.loss_penalty\n",
    "            return self.loss\n",
    "\n",
    "        self.output_layer = OutputLayer()(\n",
    "            [self.bigot_opinions, self.bigot_weights])\n",
    "\n",
    "        print('Built tree has {} leaves out of {} nodes'.format(\n",
    "            sum([node.isLeaf for node in self.nodes]), len(self.nodes)))\n",
    "\n",
    "        self.model = Model(inputs=self.input_layer, outputs=self.output_layer)\n",
    "\n",
    "        self.model.compile(optimizer=self.optimizer, loss=tree_loss,\n",
    "                           metrics=self.metrics)\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    def initialize_variables(self, sess, x, batch_size):\n",
    "        '''Since tf.ExponentialMovingAverage generates variables that\n",
    "        depend on other variables being initialized first, we need to\n",
    "        perform customized, 2-step initialization.\n",
    "        Importantly, initialization of EMA variables also requires\n",
    "        a single input batch of size that will be used for evaluation\n",
    "        of loss, in order to create compatible shapes. Therefore,\n",
    "        model will be constrained to initial batch size.\n",
    "        '''\n",
    "\n",
    "        if not self.model:\n",
    "            print('Missing model instance.')\n",
    "            return\n",
    "\n",
    "        ema_vars = [v for v in tf.global_variables() if\n",
    "                    'ExponentialMovingAverage' in v.name and\n",
    "                    'Const' not in v.name]\n",
    "        independent_vars = [v for v in tf.global_variables() if\n",
    "                            v not in ema_vars]\n",
    "        feed_dict = {self.input_layer: x[:batch_size]}\n",
    "\n",
    "        init_indep_vars_op = tf.variables_initializer(independent_vars)\n",
    "        init_ema_vars_op = tf.variables_initializer(ema_vars)\n",
    "\n",
    "        sess.run(init_indep_vars_op)\n",
    "        sess.run(init_ema_vars_op, feed_dict=feed_dict)\n",
    "        self.initialized = True\n",
    "\n",
    "    def save_variables(self, sess, path):\n",
    "        '''Keras saving methods such as model.save() or model.save_weights()\n",
    "        are not suitable, since Keras won't serialize tf.Tensor objects which\n",
    "        get included into saving process as arguments of Lambda layers.\n",
    "        '''\n",
    "        self.saver.save(sess, path)\n",
    "\n",
    "    def load_variables(self, sess, path):\n",
    "        self.saver.restore(sess, path)\n",
    "        self.initialized = True\n",
    "\n",
    "    def maybe_train(self, sess, data_train, data_valid,\n",
    "                    batch_size, epochs, callbacks=None, distill=False):\n",
    "\n",
    "        DIR_ASSETS = 'assets/'\n",
    "        DIR_DISTILL = 'distilled/'\n",
    "        DIR_NON_DISTILL = 'non-distilled/'\n",
    "        DIR_MODEL = DIR_ASSETS + (DIR_DISTILL if distill else DIR_NON_DISTILL)\n",
    "        PATH_MODEL = DIR_MODEL + 'tree-model'\n",
    "\n",
    "        try:\n",
    "            print('Loading trained model from {}.'.format(PATH_MODEL))\n",
    "            self.load_variables(sess, PATH_MODEL)\n",
    "            return\n",
    "        except ValueError as e:\n",
    "            print('{} is not a valid checkpoint. Training from scratch.'.format(\n",
    "                PATH_MODEL))\n",
    "            x_train, y_train = data_train\n",
    "            self.initialize_variables(sess, x_train, batch_size)\n",
    "            self.model.fit(\n",
    "                x_train, y_train, validation_data=data_valid,\n",
    "                batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n",
    "            print('Saving trained model to {}.'.format(PATH_MODEL))\n",
    "            if not os.path.isdir(DIR_MODEL):\n",
    "                os.mkdir(DIR_MODEL)\n",
    "            self.save_variables(sess, PATH_MODEL)\n",
    "\n",
    "    def evaluate(self, x, y, batch_size):\n",
    "        if self.model and self.initialized:\n",
    "            score = self.model.evaluate(x, y, batch_size)\n",
    "            print('accuracy: {:.2f}% | loss: {}'.format(100*score[1], score[0]))\n",
    "        else:\n",
    "            print('Missing initialized model instance.')\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.model and self.initialized:\n",
    "            return self.model.predict(x, verbose=1)\n",
    "        else:\n",
    "            print('Missing initialized model instance.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
